---
title: "Modul-11 Regresi Logistik"
author: "Roni Yunis"
date: '2022-06-09'
output:
  html_document: default
  pdf_document: default
---

Regresi Logistik adalah model statistika yang dapat digunakan untuk menganalisis pola hubungan antara sekumpulan variabel independen dengan suatu variabel dependen bertipe kategorik atau kualitatif

# Load packages
```{r}
library (dplyr)
library (ggplot2)
```

# Import Data
data yang digunakan adalah dataset diabetes.csv. Dataset ini berisikan beberapa variabel yaitu: data `Pregnancies`, `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`, `DiabetesPedigreeFunction`, `Age`, dan `Outcome`

```{r}
data <- read.csv("data/diabetes.csv")
head(data)
```
## Melihat ringkasan data

```{r}
summary(data)
```
Bisa dilihat bahwa, tidak ada data NA's dalam data yang akan digunakan

## Melihat struktur data

```{r}
glimpse(data)
```
Bisa dilihat bahwa 8 buah variabel bertipe data numerik dan ada 1 variabel (`Outcome`) yang bertipe kategorikal (kategorik). Kita asumsikan 1 = Yes, dan 0 = No

## Visualiasi data
kita akan memvisualisasi salah satu data numerik/integer, misalnya kita akan memvisualisasi variabel `DiabetePedigreeFunction` dengan menggunakan Histogram

```{r}
par(mfrow=c(4,2))
ggplot(data, aes(data[,7],fill=..count..)) + 
      geom_histogram(aes(y=..density..)) +
      geom_density(alpha=.2, fill="purple")
```
Sekarang kita akan memvisualisasi data `outcome` yang kategorikal (1 atau 0) dengan mengunakan pie chart

```{r}
YES <- sum(data$Outcome == 1)
NO <- sum(data$Outcome == 0)
slices <- c(YES,NO)
lbls <- c("Teridentifikasi","Tidak Teridentifikasi")
lbls <- paste(lbls, slices)
lbls <- paste(lbls)
pie(slices,labels = lbls, col=rainbow(length(lbls)),
    main="Pie Chart of Diabetes")
```
# Menghitung korelasi antar variabel
Dalam kasus ini kita akan menghitung kolerasi antar variabel, untuk mengetahui apakah terdapat multikolinearitas antar variabel.

```{r}
cor(data[,1:8], method="spearman")
```
Berdasarkan korelasi dari setiap variabel dapat dilihat bahwa tidak ada hubungan yang melebihi dari 0.6, sehingga dapat disimpulkan bahwa berdasarkan nilai korelasi tidak terdapat multikolinearitas antar variabel. Jika di visualisasikan dengan scatterplot hasilnya akan seperti ini:

```{r}
plot(data[,1:8], col=data$Outcome)
```
# Analisis Regresi Logistik
## Model 1

```{r}
logit1 <- glm(Outcome~., data = data, family = binomial(link="logit"))
summary(logit1)
```
Berdasarkan dari Model 1 masih ditemui variabel yang tidak berpengaruh signifikan terhadap model, karena nilai p-value > Î±=5% sehingga variabel yang tidak berpengaruh harus dihilangkan. Mengeliminasi variabel prediktor dengan cara backward. Untuk langkah selanjutnya adalah menghilangkan variabel `Age`

## Model 2

```{r}
logit2 <- glm(Outcome~Pregnancies+Glucose+BloodPressure+
                      SkinThickness+Insulin+BMI+DiabetesPedigreeFunction, data = data, family = binomial(link="logit"))
summary(logit2)
```
Pada model ini masih ditemukan variabel yang tidak signifikan, sehingga perlu dilakukan eliminasi pada variabel `Insulin`.

## Model 3

```{r}
logit3 <- glm(Outcome~Pregnancies+Glucose+BloodPressure+
                      SkinThickness+BMI+DiabetesPedigreeFunction, data = data, family = binomial(link="logit"))
summary(logit3)
```
Dalam model ke 3 ini variabel `SkinThickness` tidak berpengaruh signifikan terhadap model, sehingga perlu dihilangkan untuk mencari model selanjutnya.

## Model 4

```{r}
logit4 <- glm(Outcome~Pregnancies+Glucose+BloodPressure+
                      BMI+DiabetesPedigreeFunction, data = data, family = binomial(link="logit"))
summary(logit4)
```
Bisa dilihat bahwa pada Model 4, nilai p-value pada semua variabel sudah berpengaruh signifikan, karena nilai p-value< 0,05.Sehingga bisa dilanjutkan ketahap berikutnya

# Memilih model regresi terbaik
Metode yang dapat digunakan untuk memilih model regresi terbaik adalah metode AIC dan SIC. Model regresi terbaik apabila memiliki nilai AIC terkecil. Sekarang kita akan menghitung berapa nilai AIG dari masing-masing model tersebut.

```{r}
model <- c("Model 1","Model 2","Model 3","Model 4")
       AIC <- c(logit1$aic,logit2$aic,logit3$aic,logit4$aic)
       kriteria <- data.frame(model,AIC)
       kriteria 
```
Dari hasil output diatas dapat dilihat bahwa Model 4 merupakan model yang terbaik karena memiliki nilai AIC terkecil yaitu 740.5596.

# Goodness of Fit (Kebaikan Model)

```{r}
logitfinal <- glm(Outcome~1, data=data, family =binomial(link="logit"))
       1-as.vector(logLik(logit4)/logLik(logitfinal)) 
```
Dari hasil output diatas bisa disimpulkan bahwa Model 4 sudah memenuhi asumsi goodness of fit


```{r}
library(car)
vif(logit4)
```

Berdasarkan hasil perhitungan diperoleh nilai VIF untuk seluruh variabel prediktor dalam model 4 < 10, sehingga asumsi multikolinearitas terpenuhi (tidak terjadi multikolinearitas antar variabel). Jadi bisa disimpulkan bahwa variabel `Pregnancies`, `Glucose`, `BloodPressure`, `BMI`, dan `DiabetesPredigreeFunction` berpengaruh signifikan terhadap `Outcome`

# Latihan
Berdasarkan kasus diatas, cobalah anda lakukan klasifikasi untuk mengidentifikasi pasien yang terkena diabetes dan yang tidak terkena diabetes

```{r}
# your code



```


